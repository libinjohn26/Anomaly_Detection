# Deep Learning based Anomaly Detection using Ensembles with Leave-out classes

Anomalies are the out-of-distribution data that needs to be recognized and removed before it affects the system. Modern neural
networks are capable to classify data with high accuracy based on the knowledge it gained from the training data but are unable
to identify if the new sample belongs to in-distribution or is simply an anomaly. To address this issue, various state-of-the-art
methods exploit the knowledge of the softmax distribution of a sample by a classifier and determine if the sample is an anomaly
or not. In our work, to improve the anomaly detection performance, we exploit the strength of ensemble learning and combine
the knowledge gained by each classifier trained on different samples of data by leaving one class out at a time.

We propose two methods in this work. The first method is a statistical comparison with the help of In- and Out-of-Distribution
(OOD) reference vectors and utilizing them to determine anomalies. The second method focuses on training a binary classifier on
the dataset generated by combining the output softmax distribution of a sample by each classifier in the ensemble and subsequently
classify a given instance as an anomaly or not.

Our work includes experiments and evaluation of ensemble method with leave-out classes to detect anomalies on image
datasets (MNIST and CIFAR-10) and text datasets (20 Newsgroups). Our implementation outperforms the baseline method on the
MNIST dataset.


In the context of anomaly detection using ensembles, we try to answer the following questions:
- RQ-1: Can ensemble learning aided with different data distributions help in distinguishing anomalous data from the ones
the model was trained on?
- RQ-2: Can the knowledge gained from leaving out classes in the classifiers of an ensemble be exploited to learn more
about the anomalous data?
